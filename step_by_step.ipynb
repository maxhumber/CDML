{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature and energy demand data (MW) for the City of Toronto for the last 2 years... scraped by me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/weather_power.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature</th>\n",
       "      <th>energy_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>-16.9</td>\n",
       "      <td>5340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>-16.3</td>\n",
       "      <td>5211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>-17.6</td>\n",
       "      <td>5096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>-18.6</td>\n",
       "      <td>4987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>-17.8</td>\n",
       "      <td>4926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  temperature  energy_demand\n",
       "0  2018-01-01 00:00:00        -16.9           5340\n",
       "1  2018-01-01 01:00:00        -16.3           5211\n",
       "2  2018-01-01 02:00:00        -17.6           5096\n",
       "3  2018-01-01 03:00:00        -18.6           4987\n",
       "4  2018-01-01 04:00:00        -17.8           4926"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select and split... let's just use temperature for now (dates are a little bit more complicated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'energy_demand'\n",
    "y = df[target]\n",
    "X = df[['temperature']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Max's Tip: you should try to get to a number as quickly as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get to a number ASAP (ie: a score to beat) we'll use [`DummyRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyRegressor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "model = DummyRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DummyRegressor` will literally just predict the average for everything... doesn't matter what you put in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5692.04161844, 5692.04161844, 5692.04161844, ..., 5692.04161844,\n",
       "       5692.04161844, 5692.04161844])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm this by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5692.041618441358"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we have a model that we can score so that we know what to beat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "round(mean_squared_error(y_test, model.predict(X_test)) ** (1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that Toronto on an average day uses ~5700 MW of electricity having our model be off by ~1400 MW isn't super great! But it's a start! We're going work on continously improving this number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a prediction for a single row, or new entry, I like to send a row to a dictionary to get a sense of structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': [-6.2]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sample(1).to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then embed the output in a new pandas.DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5692.041618441358"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.DataFrame({'temperature': [21]})\n",
    "model.predict(new)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to stuff new temperatures in our app into a pandas DataFrame exactly like that. But let's save the model so that we can wrap an app around it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to building the app, let's make sure it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5692.041618441358"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.predict(new)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a \"model\" (granted it's pretty dumb!) it's time to stick it behind an app. \n",
    "\n",
    "To start our app is just going to be a shitty hello world Flask app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Hello World!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the app by running it from the command line:\n",
    "\n",
    "```\n",
    "python app.py\n",
    "```\n",
    "\n",
    "And visiting: http://127.0.0.1:5000/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrupt the process (ctrl+c) to kill the app and move on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some boilerplate in place, we can extend the hello world example to include the model. While this looks like it might work, we're going to run it anyways..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import pickle\n",
    "\n",
    "from flask import Flask\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    new = pd.DataFrame({'temperature': [20]})\n",
    "    prediction = model.predict(new)\n",
    "    print(prediction)\n",
    "    return prediction\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, fire up the app at the command line the app at the command line:\n",
    "\n",
    "```\n",
    "python app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time when you visit http://127.0.0.1:5000/ you'll get a:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TypeError** \n",
    "\n",
    "> TypeError: The view function did not return a valid response. The return type must be a string, dict, tuple, Response instance, or WSGI callable, but it was a ndarray.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrupt the process so that we can work on a fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TypeError helpful suggested that we could return a dictionary and it would work... let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import pickle\n",
    "from flask import Flask\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    new = pd.DataFrame({'temperature': [20]})\n",
    "    prediction = model.predict(new)[0]\n",
    "    # return str(prediction)\n",
    "    return {'prediction': prediction}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could `return str(prediction)`. \n",
    "\n",
    "But let's run the app at the command line and preview it once more. This time you should see the dictonary return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that while our model is embedded in our app it's just returning the prediction temperature=20 on every single request.\n",
    "\n",
    "To make our app dynamic, we'll use \"query params\"... they look like:\n",
    "\n",
    "`http://website.com/endpoint?query=string`\n",
    "\n",
    "Query params will allow us to accept different inputs and pass them to our model via a pandas DataFrame... In order to capture query params, though, we need to import `flask.request` so that we can peel off dictionary keys from the `request.args` object...\n",
    "\n",
    "As we do this, let's rejig the app to have a separate temperature endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import pickle\n",
    "from flask import Flask, request\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Use the /predict endpoint'\n",
    "\n",
    "@app.route('/predict')\n",
    "def predict():\n",
    "    query = request.args\n",
    "    print(query)\n",
    "    new = pd.DataFrame({'temperature': [20]})\n",
    "    prediction = model.predict(new)[0]\n",
    "    return {'prediction': prediction}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the app and watch the command line when you hit it with different query strings like this:\n",
    "    \n",
    "- http://127.0.0.1:5000/predict?hi=there&name=max\n",
    "- http://127.0.0.1:5000/predict?even=more&query=strings\n",
    "- http://127.0.0.1:5000/predict?temperature=25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something like `ImmutableMultiDict([('hi', 'there'), ('name', 'max')])` print to the console. Don't worry it's basically just a dictionary..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App 05 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To *actually* connect the temperature query string to the model, let's grab it off `request.args` and format it as a float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import pickle\n",
    "from flask import Flask, request\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return 'Use the /predict endpoint'\n",
    "\n",
    "@app.route('/predict')\n",
    "def predict():\n",
    "    query = request.args\n",
    "    temperature = float(query.get('temperature'))\n",
    "    print(temperature)\n",
    "    new = pd.DataFrame({'temperature': [temperature]})\n",
    "    prediction = model.predict(new)[0]\n",
    "    return {'prediction': prediction}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the app at the command line and hit the url with some temperatures:\n",
    "\n",
    "- http://127.0.0.1:5000/predict?temperature=25\n",
    "- http://127.0.0.1:5000/predict?temperature=-10\n",
    "- http://127.0.0.1:5000/predict?temperature=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the print statement in the console is registering the different values, but the model is just returning the same thing.\n",
    "\n",
    "Well, that shouldn't surprise, model is dumb! Let's fix the dummy model we started with right now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DummyRegressor` helped us quickly build an app, and gave us a number (in the form of RMSE) to beat, let's see if we can beat it with `LinearRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cae5216284c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0m\u001b[1;32m    506\u001b[0m                                    y_numeric=True, multi_output=True)\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this breaks because our data has some NaNs in the temperature column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23040 entries, 0 to 23039\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           23040 non-null  object \n",
      " 1   temperature    22873 non-null  float64\n",
      " 2   energy_demand  23040 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 540.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not to worry, dealing with NaNs is a cinch with `DataFrameMapper` from [sklearn-pandas](https://github.com/scikit-learn-contrib/sklearn-pandas) (basically my secret weapon/super power).\n",
    "\n",
    "DataFrameMapper accepts a list of tuples, where each tuple identifies the column name and then the transformer that operates on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    ('temperature', SimpleImputer())\n",
    "], df_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it works as a first class scikit-learn transformer (complete with `fit`, `transform`, and `fit_transform`) it comes with all of scikit-learns finickiness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "temperature: Expected 2D array, got 1D array instead:\narray=[-16.9 -16.3 -17.6 ...   9.1   8.7   8.3].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8172f748cb52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn_pandas/dataframe_mapper.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0my\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mrelative\u001b[0m \u001b[0mto\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn_pandas/dataframe_mapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, y, do_fit)\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0madd_column_names_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdo_fit\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdo_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn_pandas/pipeline.py\u001b[0m in \u001b[0;36m_call_fit\u001b[0;34m(fit_method, X, y, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# fit takes only one argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_indicator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0m_check_inputs_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             X = self._validate_data(X, reset=in_fit,\n\u001b[0m\u001b[1;32m    242\u001b[0m                                     \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                                     \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 )\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: temperature: Expected 2D array, got 1D array instead:\narray=[-16.9 -16.3 -17.6 ...   9.1   8.7   8.3].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "mapper.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above hits a **Value Error**\n",
    "\n",
    "> ValueError: temperature: Expected 2D array, got 1D array instead:\n",
    "\n",
    "Because of how scikit-learn is designed. Without going into the nitty-gritty, this error has to do with the difference between how scikit-learn handles strings and numbers and columns and series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not to worry, errors like this in DataFrameMapper are easily fixed by wrapping the pesky column in square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-16.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-16.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20731</th>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20732</th>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20733</th>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20734</th>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20735</th>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20736 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature\n",
       "0            -16.9\n",
       "1            -16.3\n",
       "2            -17.6\n",
       "3            -18.6\n",
       "4            -17.8\n",
       "...            ...\n",
       "20731         10.3\n",
       "20732          9.5\n",
       "20733          9.1\n",
       "20734          8.7\n",
       "20735          8.3\n",
       "\n",
       "[20736 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    (['temperature'], SimpleImputer())\n",
    "], df_out=True)\n",
    "\n",
    "mapper.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the DataFrameMapper, we can now transform our `X` objects to intermediate `Z` objects... \n",
    "\n",
    "> Max's Tip: You could just overwrite the `X`s but I like `Z`s becauxe they remind me \"HEY I DID SOMETHING TO THIS!~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_train = mapper.fit_transform(X_train)\n",
    "Z_test = mapper.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's score the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(mean_squared_error(y_test, model.predict(Z_test)) ** (1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And notice that this new model beats the dummy, albeit by not much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peeking at some examples we can see that it's still pretty constrained around the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22211</th>\n",
       "      <td>7358</td>\n",
       "      <td>5926.583433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21302</th>\n",
       "      <td>5910</td>\n",
       "      <td>5925.108093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21171</th>\n",
       "      <td>3835</td>\n",
       "      <td>5724.461788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21114</th>\n",
       "      <td>5782</td>\n",
       "      <td>5821.834260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21492</th>\n",
       "      <td>4798</td>\n",
       "      <td>5811.506876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22571</th>\n",
       "      <td>8060</td>\n",
       "      <td>5959.040924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>6435</td>\n",
       "      <td>5917.731390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22591</th>\n",
       "      <td>6236</td>\n",
       "      <td>5860.193112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22154</th>\n",
       "      <td>4701</td>\n",
       "      <td>5874.946517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21276</th>\n",
       "      <td>7419</td>\n",
       "      <td>5935.435476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y_true        y_hat\n",
       "22211    7358  5926.583433\n",
       "21302    5910  5925.108093\n",
       "21171    3835  5724.461788\n",
       "21114    5782  5821.834260\n",
       "21492    4798  5811.506876\n",
       "22571    8060  5959.040924\n",
       "21585    6435  5917.731390\n",
       "22591    6236  5860.193112\n",
       "22154    4701  5874.946517\n",
       "21276    7419  5935.435476"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'y_true': y_test,\n",
    "    'y_hat': model.predict(Z_test)\n",
    "}).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our model still isn't amazing, it is an improvement, and a little more dynamic, so let's  \"serialize\" the mapper and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mapper.pkl', 'wb') as f:\n",
    "    pickle.dump(mapper, f)\n",
    "    \n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, wait up, this isn't super ideal because in this paradigm we have to keep track of and load two separate things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mapper.pkl', 'rb') as f:\n",
    "    mapper = pickle.load(f)\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in order to predict we need to remember to transform first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5879.372538053129"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.DataFrame({'temperature': [21]})\n",
    "\n",
    "Z_new = mapper.transform(new)\n",
    "model.predict(Z_new)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually make things a little more simple..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter pipelines... a scikit-learn tool that will ensure that we only have to manage one thing!\n",
    "\n",
    "And, as an added bonus, using pipeline will make it so that we can get rid of the intermediate `Z` objects at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(mapper, model)\n",
    "pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only have to dump one thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load one object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pkl', 'rb') as f:\n",
    "    pipe = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions don't require transformation now (because it happens inside the pipeline): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5879.372538053129"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(new)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deploy this new pipeline model let's wrap everything into a single file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "df = pd.read_csv('data/weather_power.csv')\n",
    "\n",
    "target = 'energy_demand'\n",
    "y = df[target]\n",
    "X = df[['temperature']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    (['temperature'], SimpleImputer())\n",
    "], df_out=True)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "pipe = make_pipeline(mapper, model)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "with open('pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy 01 - Heroku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to deploy our model and app on Heroku. In order to do that, follow these simple steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup a virtual environment:\n",
    "\n",
    "```\n",
    "python -m venv .venv\n",
    "```\n",
    "\n",
    "2. Activate it:\n",
    "\n",
    "```\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "3. Install the app and model dependencies (`gunicorn` something that will sit between heroku and our app...):\n",
    "\n",
    "```\n",
    "pip install gunicorn flask scikit-learn pandas sklearn_pandas\n",
    "```\n",
    "\n",
    "3. Freeze the dependencies:\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "4. Retrain the model inside of the virtual environment:\n",
    "\n",
    "```\n",
    "python model.py\n",
    "```\n",
    "\n",
    "5. Make sure the app still works locally:\n",
    "\n",
    "```\n",
    "python app.py\n",
    "```\n",
    "\n",
    "6. Specify a python runtime (3.8 not working yet):\n",
    "\n",
    "```\n",
    "python --version\n",
    "echo \"python-3.7.9\" > runtime.txt\n",
    "```\n",
    "\n",
    "7. Create a `Procfile`:\n",
    "\n",
    "```\n",
    "echo \"web: gunicorn app:app\" > Procfile\n",
    "```\n",
    "\n",
    "8. (Optional) If your project isn't already a git repo, make it one:\n",
    "\n",
    "```\n",
    "git init\n",
    "touch .gitignore\n",
    "echo \".venv\" >> .gitignore\n",
    "```\n",
    "\n",
    "9. Login to Heroku from the [command line](https://devcenter.heroku.com/articles/heroku-cli):\n",
    "\n",
    "```\n",
    "heroku login\n",
    "```\n",
    "\n",
    "10. Create a project:\n",
    "\n",
    "```\n",
    "heroku create\n",
    "```\n",
    "\n",
    "11. Add a remote to the randomly generated project:\n",
    "\n",
    "```\n",
    "heroku git:remote -a silly-words-009900\n",
    "```\n",
    "\n",
    "12. Test the app locally:\n",
    "\n",
    "```\n",
    "heroku local\n",
    "```\n",
    "\n",
    "13. add, commit push:\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m 'ðŸš€'\n",
    "git push heroku master\n",
    "```\n",
    "\n",
    "14. Hit the url and make sure it works!\n",
    "\n",
    "- http://\\<url\\>/predict?temperature=20\n",
    "\n",
    "15. Make sure nothing is wrong (check the logs!):\n",
    "\n",
    "```\n",
    "heroku logs -t \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastAPI is the new kid on the block. It's faster, less boilerplate, and the future. Converting from Flask to FastAPI isn't that tough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "with open('pipe.pkl', 'rb') as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "@app.get('/')\n",
    "def index():\n",
    "    return 'Use the /predict endpoint with a temperature argument'\n",
    "\n",
    "@app.get('/predict')\n",
    "def predict(temperature: float):\n",
    "    new = pd.DataFrame({'temperature': [temperature]})\n",
    "    prediction = pipe.predict(new)[0]\n",
    "    return {'prediction': prediction}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run at the command line with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uvicorn app:app --port 5000 --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out and make sure it still works:\n",
    "\n",
    "- http://127.0.0.1:5000/predict?temperature=20\n",
    "\n",
    "As a bonus there's also some wicked auto-generate docs at:\n",
    "\n",
    "- http://127.0.0.1:5000/docs\n",
    "\n",
    "Interrupt and kill when you've verified it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heroku + fastAPI\n",
    "\n",
    "0. If you're not in your environment anymore you can re-enter with:\n",
    "\n",
    "`source .venv/bin/activate`\n",
    "\n",
    "(And to exit out to your base environment):\n",
    "\n",
    "`deactivate`\n",
    "\n",
    "1. Install the new dependencies:\n",
    "\n",
    "```\n",
    "pip install uvicorn fastapi\n",
    "```\n",
    "\n",
    "2. Freeze the dependencies:\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "3. Retrain the model inside of the virtual environment:\n",
    "\n",
    "```\n",
    "python model.py\n",
    "```\n",
    "\n",
    "4. Make sure the app still works locally:\n",
    "\n",
    "```\n",
    "uvicorn app:app --port=5000\n",
    "```\n",
    "\n",
    "5. Create a new `Procfile`:\n",
    "\n",
    "```\n",
    "echo \"web: uvicorn app:app --host=0.0.0.0 --port=${PORT:-5000}\" > Procfile\n",
    "```\n",
    "\n",
    "6. Test the app locally:\n",
    "\n",
    "```\n",
    "heroku local\n",
    "```\n",
    "\n",
    "7. add, commit push:\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m 'ðŸš€'\n",
    "git push heroku master\n",
    "```\n",
    "\n",
    "8. Click on the url and make sure it works!\n",
    "\n",
    "- http://\\<url\\>/predict?temperature=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we have right now sucks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(range(len(y_test)), y_test)\n",
    "plt.plot(range(len(y_test)), model.predict(Z_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It barely explains anything... but it actually turns temperature can explain a lot when we properly handle the column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['temperature'], df['energy_demand'], alpha=1/20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we see a \"U\" or a rainbow, we should look at brining in Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    (['temperature'], [SimpleImputer(), PolynomialFeatures(degree=2, include_bias=False)])\n",
    "], df_out=True)\n",
    "\n",
    "mapper.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(mapper, model)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(mean_squared_error(y_test, pipe.predict(X_test)) ** (1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explained over half of the variance and have our model off by < 1000 mw. Look at the updated prediction curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still have a lot of work to do, but let's wrap it up in a `model.py` file so that we can redeploy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df = pd.read_csv('data/weather_power.csv')\n",
    "\n",
    "target = 'energy_demand'\n",
    "y = df[target]\n",
    "X = df[['temperature']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    (['temperature'], [SimpleImputer(), PolynomialFeatures(degree=2, include_bias=False)])\n",
    "], df_out=True)\n",
    "\n",
    "model = LinearRegression()\n",
    "pipe = make_pipeline(mapper, model)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "with open('pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No dependencies have changed, so it's a lot easier to push a change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Retrain the model inside of the virtual environment:\n",
    "\n",
    "```\n",
    "python model.py\n",
    "```\n",
    "\n",
    "2. Test the app locally:\n",
    "\n",
    "```\n",
    "heroku local\n",
    "```\n",
    "\n",
    "3. add, commit push:\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m 'ðŸš€'\n",
    "git push heroku master\n",
    "```\n",
    "\n",
    "4. Click on the url and make sure it works!\n",
    "\n",
    "- http://\\<url\\>/predict?temperature=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 08 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now our model is just using temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(y_test)), y_test)\n",
    "plt.plot(range(len(y_test)), pipe.predict(Z_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has to be so relationship betwen date. And hour. Let's convert the date column from a string to a date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could import the data with the date column (0th position), already parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/weather_power.csv', parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably want month, weekday and hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = df['date']\n",
    "\n",
    "pd.concat([col.dt.month, col.dt.weekday, col.dt.hour], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recut the X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'energy_demand'\n",
    "y = df[target]\n",
    "X = df[['date', 'temperature']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a scikit-learn transformer to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DateEncoder(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.concat([X.dt.month, X.dt.weekday, X.dt.hour], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That way we can apply it on the `X_train` and `X_test` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DateEncoder().fit_transform(X_train['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can easily embed it in our mapper framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([\n",
    "    ('date', DateEncoder(), {'input_df': True}),\n",
    "    (['temperature'], [SimpleImputer(), PolynomialFeatures(degree=2, include_bias=False)])\n",
    "], df_out=True)\n",
    "\n",
    "model = LinearRegression()\n",
    "pipe = make_pipeline(mapper, model)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explains even more variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And shaving off more from RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, pipe.predict(X_test)) ** (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(y_test)), y_test)\n",
    "plt.plot(range(len(y_test)), pipe.predict(X_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way pickle works, we have move the `DateEncoder` to a `utils.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DateEncoder(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.concat([X.dt.month, X.dt.weekday, X.dt.hour], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the complete model to a new file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from utils import DateEncoder # CUSTOM IMPORT\n",
    "\n",
    "df = pd.read_csv('data/weather_power.csv', parse_dates=[0])\n",
    "\n",
    "target = 'energy_demand'\n",
    "y = df[target]\n",
    "X = df[['date', 'temperature']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    ('date', DateEncoder(), {'input_df': True}),\n",
    "    (['temperature'], [SimpleImputer(), PolynomialFeatures(degree=2, include_bias=False)])\n",
    "], df_out=True)\n",
    "\n",
    "model = LinearRegression()\n",
    "pipe = make_pipeline(mapper, model)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "with open('pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App 08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the custom `DateEncoder` and accept a dictionary in the index, and change it to a post request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "from typing import Dict\n",
    "import os\n",
    "from utils import DateEncoder\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "with open('pipe.pkl', 'rb') as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "@app.post('/')\n",
    "def index(json_data: Dict):\n",
    "    new = pd.DataFrame({\n",
    "        'date': [pd.Timestamp(json_data.get('date'))],\n",
    "        'temperature': [float(json_data.get('temperature'))]\n",
    "    })\n",
    "    prediction = pipe.predict(new)[0]\n",
    "    return {'prediction': prediction}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Carrier Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a post carrier to hit the local endpoint and when it's deployed, this is the function we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "\n",
    "def post(url, data):\n",
    "    data = bytes(json.dumps(data).encode(\"utf-8\"))\n",
    "    request = Request(\n",
    "        url=url,\n",
    "        data=data,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-type\", \"application/json; charset=UTF-8\")\n",
    "    with urlopen(request) as response:\n",
    "        data = json.loads(response.read().decode(\"utf-8\"))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy on Dokku (Heroku Open Source, and better)\n",
    "\n",
    "Create a new Procfile (respect port 5000):\n",
    "\n",
    "`echo \"web: uvicorn app:app --host=0.0.0.0 --port=${PORT:-5000}\" > Procfile`\n",
    "\n",
    "\n",
    "Rerun the model:\n",
    "\n",
    "```\n",
    "python model.py\n",
    "```\n",
    "\n",
    "Make sure the app works locally:\n",
    "\n",
    "```\n",
    "uvicorn app:app --port 5000 --reload\n",
    "```\n",
    "\n",
    "Use the post function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"date\": str(pd.Timestamp('now')),\n",
    "    \"temperature\": 25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post(\"http://127.0.0.1:5000\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've confirmed that it still works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy Dokku by:\n",
    "\n",
    "1. Sign up for a [DigitalOcean](https://m.do.co/c/2909cd1f3f10) account\n",
    "\n",
    "2. Spin up a $5 Ubuntu 20/18.04 server...\n",
    "\n",
    "3. ssh into it:\n",
    "\n",
    "```\n",
    "ssh root@165.XXX.43.118\n",
    "```\n",
    "\n",
    "4. (Strongly advised) Update everything:\n",
    "\n",
    "```\n",
    "sudo apt update\n",
    "sudo apt -y upgrade\n",
    "```\n",
    "\n",
    "5. Setup firewall:\n",
    "\n",
    "````\n",
    "ufw app list\n",
    "ufw allow OpenSSH\n",
    "ufw enable\n",
    "````\n",
    "\n",
    "6. Add some rules ([source](https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-with-ufw-on-ubuntu-18-04)):\n",
    "\n",
    "```\n",
    "sudo ufw default deny incoming\n",
    "sudo ufw default allow outgoing\n",
    "sudo ufw allow ssh\n",
    "sudo ufw allow 22\n",
    "sudo ufw allow http\n",
    "sudo ufw allow https\n",
    "```\n",
    "\n",
    "7. Install dokku:\n",
    "\n",
    "```\n",
    "wget https://raw.githubusercontent.com/dokku/dokku/v0.21.4/bootstrap.sh\n",
    "sudo DOKKU_TAG=v0.21.4 bash bootstrap.sh\n",
    "```\n",
    "\n",
    "**THIS STEP IS IMPORTANT!**\n",
    "\n",
    "8. Visit the Dropletâ€™s IP address in a browser to finish configuring Dokku\n",
    "\n",
    "\n",
    "9. Copy and paste your ssh key from your **laptop** into the config window:\n",
    "\n",
    "```\n",
    "cat .ssh/id_rsa.pub\n",
    "```\n",
    "\n",
    "10. And add the IP of the server to the hostname:\n",
    "\n",
    "`68.183.XXX.31`\n",
    "\n",
    "\n",
    "11. Click \"Finish Setup\"...\n",
    "\n",
    "\n",
    "12. Go back to the server terminal and create a dokku app on the server:\n",
    "\n",
    "```\n",
    "dokku apps:create powerapp\n",
    "dokku domains:enable powerapp\n",
    "\n",
    "```\n",
    "\n",
    "**On Laptop**\n",
    "\n",
    "12. Add dokku as a remote:\n",
    "\n",
    "```\n",
    "git remote add dokku dokku@165.XXX.43.118:powerapp\n",
    "```\n",
    "\n",
    "13. Verify that the remote got added:\n",
    "\n",
    "```\n",
    "git remote -v\n",
    "```\n",
    "\n",
    "14. Push it up (for every new change just run these commands):\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m 'ðŸ¤ž'\n",
    "git push dokku master\n",
    "```\n",
    "\n",
    "15. Test if it works with the post function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post(\"http://142.93.148.110\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    ('date', DateEncoder(), {'input_df': True}),\n",
    "    (['temperature'], [SimpleImputer(), PolynomialFeatures(degree=2, include_bias=False)])\n",
    "], df_out=True)\n",
    "\n",
    "Z_train = mapper.fit_transform(X_train)\n",
    "Z_test = mapper.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(Z_train.shape[1],)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='adam',\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    Z_train, y_train,\n",
    "    epochs=100, batch_size=32,\n",
    "    validation_data=(Z_test, y_test)\n",
    ")\n",
    "\n",
    "model.evaluate(Z_test, y_test)\n",
    "r2_score(y_test, model.predict(Z_test)), r2_score(y_test, model.predict(Z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict something new:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame({\n",
    "    'date': [pd.Timestamp('now')],\n",
    "    'temperature': [17]\n",
    "})\n",
    "\n",
    "model.predict(mapper.transform(new))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(X_test['date'], y_test, alpha=1/2);\n",
    "plt.plot(X_test['date'], model.predict(Z_test).flatten(), alpha=1/2);\n",
    "\n",
    "round(mean_squared_error(y_test, model.predict(Z_test)) ** (1/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a KerasRegressor Wrapper and a SelectKBest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "Z_train = mapper.fit_transform(X_train)\n",
    "Z_test = mapper.transform(X_test)\n",
    "\n",
    "columns = 5\n",
    "select = SelectKBest(k=columns)\n",
    "\n",
    "select.fit_transform(Z_train, y_train)\n",
    "\n",
    "def nn():\n",
    "    columns = 5\n",
    "    m = Sequential()\n",
    "    m.add(Input(shape=(columns,)))\n",
    "    m.add(Dense(10, activation='relu'))\n",
    "    m.add(Dense(10, activation='relu'))\n",
    "    m.add(Dense(1))\n",
    "    m.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer='adam',\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return m\n",
    "\n",
    "model = KerasRegressor(nn, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the pipe can work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(mapper, select, model)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "new = pd.DataFrame({\n",
    "    'date': [pd.Timestamp('now')],\n",
    "    'temperature': [17]\n",
    "})\n",
    "\n",
    "float(pipe.predict(new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing is going to be a bit of a headache, need to dump the model first then pickle the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.named_steps['kerasregressor'].model.save('model.h5')\n",
    "pipe.named_steps['kerasregressor'].model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading it back in looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pipe.pkl', 'rb') as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "pipe.named_steps['kerasregressor'].model = load_model('model.h5')\n",
    "\n",
    "float(pipe.predict(new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should move the `nn` definition to utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "class DateEncoder(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.concat([X.dt.month, X.dt.weekday, X.dt.hour], axis=1)\n",
    "    \n",
    "def nn():\n",
    "    columns = 5\n",
    "    m = Sequential()\n",
    "    m.add(Input(shape=(columns,)))\n",
    "    m.add(Dense(10, activation='relu'))\n",
    "    m.add(Dense(10, activation='relu'))\n",
    "    m.add(Dense(1))\n",
    "    m.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer='adam',\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from utils import DateEncoder, nn\n",
    "\n",
    "df = pd.read_csv('data/weather_power.csv', parse_dates=[0])\n",
    "\n",
    "target = 'energy_demand'\n",
    "y = df[target]\n",
    "X = df[['date', 'temperature']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "    ('date', DateEncoder(), {'input_df': True}),\n",
    "    (['temperature'], [SimpleImputer(), PolynomialFeatures(degree=2, include_bias=False)])\n",
    "], df_out=True)\n",
    "\n",
    "columns = 5\n",
    "select = SelectKBest(k=columns)\n",
    "\n",
    "model = KerasRegressor(nn, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "pipe = make_pipeline(mapper, select, model)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "pipe.named_steps['kerasregressor'].model.save('model.h5')\n",
    "pipe.named_steps['kerasregressor'].model = None\n",
    "\n",
    "with open('pipe.pkl', 'wb') as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the tensorflow model and async and RequestData:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "from typing import Dict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from utils import DateEncoder, nn\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "with open('pipe.pkl', 'rb') as f:\n",
    "    pipe = pickle.load(f)\n",
    "\n",
    "pipe.named_steps['kerasregressor'].model = load_model('model.h5')\n",
    "\n",
    "class RequestData(BaseModel):\n",
    "    date: str\n",
    "    temperature: float\n",
    "\n",
    "@app.post('/')\n",
    "async def index(request: RequestData): # add async and RequestData\n",
    "    new = pd.DataFrame({\n",
    "        'date': [pd.Timestamp(request.date)],\n",
    "        'temperature': [request.temperature]\n",
    "    })\n",
    "    prediction = float(pipe.predict(new))\n",
    "    return {'prediction': prediction}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Update environment:\n",
    "\n",
    "```\n",
    "pip install tensorflow\n",
    "```\n",
    "\n",
    "2. Freeze the dependencies:\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "3. Retrain the model inside of the virtual environment:\n",
    "\n",
    "```\n",
    "python model.py\n",
    "```\n",
    "\n",
    "4. Make sure the app still works locally:\n",
    "\n",
    "```\n",
    "uvicorn app:app --port 5000 --reload\n",
    "```\n",
    "\n",
    "5. Push everything up to GitHub:\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m 'ðŸš€'\n",
    "git push dokku\n",
    "```\n",
    "\n",
    "6. Check logs on server (to make sure it all works)\n",
    "\n",
    "```\n",
    "dokku logs powerapp --tail\n",
    "```\n",
    "\n",
    "7. Test with the post function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"date\": str(pd.Timestamp('now')),\n",
    "    \"temperature\": 20\n",
    "}\n",
    "\n",
    "post(\"http://142.93.148.110\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "\n",
    "New changes that don't have any new depends will just need:\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m 'ðŸš€'\n",
    "git push dokku\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
